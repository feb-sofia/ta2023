{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T14:52:49.096852981Z",
     "start_time": "2023-11-27T14:52:49.056190916Z"
    }
   },
   "id": "46fa93ca6b813111"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Text Classification with TF-IDV"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e353de43d740959d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As machine learning algorithms can only deal with numeric data we need to find a way to map\n",
    "text documents to a representation in a numeric space. The traditional approach to create such\n",
    "a map has been to create a vocabulary of the words (or a subset of the words) encountered in a corpus and\n",
    "map each word to a one hot encoded vectors of the dimension of the vocabulary.\n",
    "\n",
    "Consider a very small example of three documents containing a tiny vocabulary of just three words\n",
    "\n",
    "$$\n",
    "\\text{doc1} = [\\text{boy}, \\text{mouse}] \\\\\n",
    "\\text{doc2} = [\\text{mouse}] \\\\\n",
    "\\text{doc3} = [\\text{zoo}, \\text{boy}]\\\\\n",
    "\\text{doc4} = [\\text{zoo}, \\text{boy}, \\text{zoo}]\n",
    "$$\n",
    "\n",
    "Consider a very simple example with a vocabulary \\[\"boy\", \"mouse\", \"zoo\"\\]. One hot encoding each word will result in the following three vectors\n",
    "\n",
    "$$\n",
    "\\text{boy} \\to\n",
    "\\left(\n",
    "    \\begin{array}{c}\n",
    "        1 \\\\\n",
    "        0\\\\\n",
    "        0\n",
    "    \\end{array}\n",
    "\\right)\n",
    "\\quad\n",
    "\\text{mouse} \\to\n",
    "\\left(\n",
    "    \\begin{array}{c}\n",
    "        0 \\\\\n",
    "        1\\\\\n",
    "        0\n",
    "    \\end{array}\n",
    "\\right)\n",
    "\\quad\n",
    "\\text{zoo} \\to\n",
    "\\left(\n",
    "    \\begin{array}{c}\n",
    "        0 \\\\\n",
    "        0\\\\\n",
    "        1\n",
    "    \\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Depending on how we combine the word vectors to a document vector.\n",
    "There are numerous possibilities to represent the documents in this three-dimensional space. A simple option is to sum the vectors in each document. This is what `CountVectorizer` does: the result is a $3 \\times 3$ matrix."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85da08acd3a39f1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "toy_corpus = [\n",
    "    \"boy mouse boy\",\n",
    "    \"mouse\",\n",
    "    \"zoo boy\",\n",
    "    \"zoo boy zoo\"\n",
    "]\n",
    "\n",
    "c_vect = CountVectorizer()\n",
    "term_matrix = c_vect.fit_transform(toy_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c_vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Check the vocabulary learned from the corpus\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mc_vect\u001B[49m\u001B[38;5;241m.\u001B[39mvocabulary_\n",
      "\u001B[0;31mNameError\u001B[0m: name 'c_vect' is not defined"
     ]
    }
   ],
   "source": [
    "# Check the vocabulary learned from the corpus\n",
    "c_vect.vocabulary_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T14:51:13.021767144Z",
     "start_time": "2023-11-27T14:51:12.806697309Z"
    }
   },
   "id": "da0c613eca0e719f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the extracted features\n",
    "c_vect.get_feature_names_out()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fc5d4c3667effcc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Check the document representation\n",
    "term_matrix_dense = term_matrix.toarray()\n",
    "pd.DataFrame(term_matrix_dense,\n",
    "             columns=c_vect.get_feature_names_out(),\n",
    "             index=[f\"doc{i}\" for i in range(1, len(toy_corpus) + 1)]\n",
    "             )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "626a85b63ff31779"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TF-IDF \n",
    "\n",
    "Often another aggregation of the word vectors is preferred, where\n",
    "the counts are weighted by the inverse document frequency.\n",
    "\n",
    "**Term frequency** ($TF(i)$) is the number of occurrences of word $i$ in document $D$. It depends strongly\n",
    "on how general a word is (e.g. \"has\" vs. \"cosine\" in general literature) and also on the length of the document.\n",
    "\n",
    "**Document frequency** ($DF(i)$ is the number of documents that contain word $i$.\n",
    "\n",
    "**Inverse document frequency** ($IDF(i)$ is simply the inverse relative frequency of the word in the set of documents.\n",
    "With $N$ documents the IDF is given by:\n",
    "\n",
    "$$\n",
    "    IDF(i) = \\frac{N}{DF(i)}\n",
    "$$\n",
    "\n",
    "It is large for words that occur in many documents, and it will be small for words that appear in only a few documents.\n",
    "\n",
    "A problem with this definition is that the IDF becomes very large for large corpora (large N) so it is commonly replaced\n",
    "by its logarithm.\n",
    "\n",
    "$$\n",
    "    IDF(i) = 1 + \\log\\left(\\frac{N}{DF(i)}\\right)\n",
    "$$\n",
    "\n",
    "The addition of 1 in the above equation serves to ensure that the words that occur in all documents are not entirely discarded. The default IDF used in `TfidfVectorizer` is:\n",
    "\n",
    "$$\n",
    "    IDF(i) = 1 + \\log\\left(\\frac{N + 1}{DF(i) + 1}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\text{TF-IDF}(i, d) = TF(i, d) \\times IDF(i)\n",
    "$$\n",
    "\n",
    "Let us calculate the document frequencies from the term density matrix we just created:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c82b78295196faf6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "aea4a573f8b93c6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First we limit the elements of the matrix to a maximum of 1,\n",
    "# and then sum the matrix column-wise\n",
    "doc_frequencies = np.clip(term_matrix_dense, None, 1).sum(axis=0)\n",
    "\n",
    "pd.DataFrame(doc_frequencies, index=c_vect.get_feature_names_out())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "444b9b21b6521066"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2231435513142097\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(math.log((4 + 1) / (3 + 1)) + 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T14:53:28.997305945Z",
     "start_time": "2023-11-27T14:53:28.956287145Z"
    }
   },
   "id": "ace38c344c132bf0"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m tfidf_vect \u001B[38;5;241m=\u001B[39m \u001B[43mTfidfVectorizer\u001B[49m(smooth_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, use_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m      2\u001B[0m tfidf_term_matrix \u001B[38;5;241m=\u001B[39m tfidf_vect\u001B[38;5;241m.\u001B[39mfit_transform(toy_corpus)\n\u001B[1;32m      3\u001B[0m pd\u001B[38;5;241m.\u001B[39mDataFrame(tfidf_term_matrix\u001B[38;5;241m.\u001B[39mtoarray(),\n\u001B[1;32m      4\u001B[0m              columns\u001B[38;5;241m=\u001B[39mc_vect\u001B[38;5;241m.\u001B[39mget_feature_names_out(),\n\u001B[1;32m      5\u001B[0m              index\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdoc\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mlen\u001B[39m(toy_corpus) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)]\n\u001B[1;32m      6\u001B[0m              )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(smooth_idf=True, use_idf=True, norm=None)\n",
    "tfidf_term_matrix = tfidf_vect.fit_transform(toy_corpus)\n",
    "pd.DataFrame(tfidf_term_matrix.toarray(),\n",
    "             columns=c_vect.get_feature_names_out(),\n",
    "             index=[f\"doc{i}\" for i in range(1, len(toy_corpus) + 1)]\n",
    "             )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T14:53:38.286884098Z",
     "start_time": "2023-11-27T14:53:38.256221216Z"
    }
   },
   "id": "fec61986b6371a7b"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Get the inverse document frequency\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtfidf_vect\u001B[49m\u001B[38;5;241m.\u001B[39midf_\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tfidf_vect' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the inverse document frequency\n",
    "tfidf_vect.idf_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T14:54:07.341110626Z",
     "start_time": "2023-11-27T14:54:07.297816488Z"
    }
   },
   "id": "4a98b43c87c4fa4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
