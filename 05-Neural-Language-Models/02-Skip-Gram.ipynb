{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c24fded7db0e8f68",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# The Neural Bi-gram Model\n",
    "\n",
    "The logistic regression model that we have considered this far has several considerable disadvantages.\n",
    "\n",
    "It is slow and expensive to train, despite the fact that it is a linear model. Condider the $V \\times V$ weights matrix. It has $V^2$ parameters. For a vocabulary of 100,000 words, this is 10 billion parameters. This is a lot of parameters to estimate, and it is not surprising that it takes a long time to train.\n",
    "  \n",
    "\n",
    "We can try to work around this by compressing the inputs into a lower dimensional space, which is exactly what neural networks do. We can think of the neural network as a non-linear function that maps the inputs into a lower dimensional space. The neural network is then a linear model in this lower dimensional space.\n",
    "\n",
    "![Hidden Layer Network](03-one-hidden-layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16234e84c8e63a5f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This model will have two sets of weights: $W^{(2)}$ ($V \\times D)$ and $W^{(1)}$ ($D \\times V)$. Choosing $D$ to be much smaller than $V$ will reduce the number of parameters that we need to estimate. With $D = 100$ and $V = 10000$ we have 1 million parameters, which is a lot less than 10 billion.\n",
    "\n",
    "Let's define the neural network model:\n",
    "The hidden layer will have $D = 30$ neurons and a $tanh$ activation.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{H} = tanh(\\mathbf{X} \\mathbf{W^{(2)}}) \\\\\n",
    "\\hat{Y} = softmax(\\mathbf{H}\\mathbf{W^{(1)}})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The loss function is the same as before:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{W}^h, \\mathbf{W}^o) = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^V Y_{ij} \\log \\hat{Y}_{ij}\n",
    "$$\n",
    "\n",
    "The weights are updated using gradient descent:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{W}^{h, \\text{new}} = \\mathbf{W}^{h, \\text{old}} - \\eta \\nabla_{\\mathbf{W}^h} J \\\\\n",
    "\\mathbf{W}^{o, \\text{new}} = \\mathbf{W}^{o, \\text{old}} - \\eta \\nabla_{\\mathbf{W}^o} J \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The gradients are computed using the chain rule and can be shown to be:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\mathbf{W}^o} J = \\mathbf{H}^T (\\hat{Y} - Y) \\\\\n",
    "\\nabla_{\\mathbf{W}^h} J = \\mathbf{X}^T (\\hat{Y} - Y) \\odot (1 - \\mathbf{H}^2) \\mathbf{W}^{oT}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the above, the $\\odot$ operator is the element-wise product of two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e29936834430c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T16:04:04.743763007Z",
     "start_time": "2023-12-20T16:04:04.514879793Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As before, we start by producing the training data\n",
    "import numpy as np\n",
    "\n",
    "# This is the same softmax function as before\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "def train_neural_bigram(sentences: list, V: int, D: int = 100, learning_rate: float = 0.01, epochs: int = 100):\n",
    "    \"\"\"\n",
    "    Train a neural bigram model with tanh activation function\n",
    "    \n",
    "    :param sentences: A list sentences. Each sentence is a list of integers corresponding to the indices of the words in the vocabulary\n",
    "    :param V: The size of the vocabulary\n",
    "    :param D: The size of the hidden layer (size of the word vectors)\n",
    "    :param learning_rate: The learning rate of the gradient descent algorithm\n",
    "    :param epochs: Number of epochs to train the model\n",
    "    :return: Hidden and output weights and a list of losses at each iteration\n",
    "    \"\"\"\n",
    "    # initialize weights\n",
    "    Wh = np.random.randn(V, D) / np.sqrt(V)\n",
    "    Wo = np.random.randn(D, V) / np.sqrt(D)\n",
    "    \n",
    "    # A list to store the loss at each iteration\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # shuffle sentences at each epoch\n",
    "        np.random.shuffle(sentences)\n",
    "        \n",
    "        j = 0 # keep track of iterations\n",
    "        for sentence in sentences:\n",
    "            # convert sentence into one-hot encoded inputs and targets\n",
    "            n = len(sentence)\n",
    "            inputs = np.zeros((n - 1, V))\n",
    "            targets = np.zeros((n - 1, V))\n",
    "            inputs[np.arange(n - 1), sentence[:n-1]] = 1\n",
    "            targets[np.arange(n - 1), sentence[1:]] = 1\n",
    "            \n",
    "            # get output predictions\n",
    "            hidden = np.tanh(inputs.dot(Wh))\n",
    "            p = softmax(hidden.dot(Wo))\n",
    "            \n",
    "            # do a gradient descent step\n",
    "            W2 = Wo - learning_rate * hidden.T.dot(p - targets)\n",
    "            dhidden = (p - targets).dot(W2.T) * (1 - hidden * hidden)\n",
    "            Wh = Wh - learning_rate * inputs.T.dot(dhidden)\n",
    "            \n",
    "            # keep track of the loss\n",
    "            loss = -np.sum(targets * np.log(p)) / (n - 1)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            if j % 10 == 0:\n",
    "                print(\"epoch:\", epoch, \"sentence: %s/%s\" % (j, len(sentences)), \"loss:\", loss)\n",
    "            j += 1\n",
    "            \n",
    "    return Wh, Wo, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Skip-Gram Model\n",
    "\n",
    "In the previous model, we used the first word in a bi-gram to predict the second word. \n",
    "\n",
    "$$\n",
    "P(w_{t+1} | w_t) = \\text{softmax}(\\mathbf{W}^o^T f(\\mathbf{W}^hT x_t))\n",
    "$$\n",
    "\n",
    "The model already produces word vectors (embeddings) for each word in the vocabulary. A problem with the model, however, is that these\n",
    "word vectors perform poorly on word similarity and other NLP tasks, because they are trained to predict the next word in a sequence, nothing else.\n",
    "\n",
    "The skip-gram model is a modification of the bi-gram model that produces better word vectors. Instead of predicting the next word, it uses the current word to predict its surrounding words (context window).\n",
    "\n",
    "Furthermore, it uses the dot product between the sets of weights (word vectors) to compute the probability of the surrounding words. The dot-product is related to the similarity (cosine) of the word vectors.\n",
    "\n",
    "\n",
    "Let's look at an example sentence:\n",
    "\n",
    "```\n",
    "Alice was beginning to get very **tired** of sitting by her sister on the bank.\n",
    "```\n",
    "\n",
    "The bi-gram model would produce the following training data the word \"tired\":\n",
    "\n",
    "- (tired, of)\n",
    "\n",
    "The skip-gram model looks at a set of nearby words (context window). For a context window of size $w = 2$, the skip-gram will produce the following training data for the word tired:\n",
    "\n",
    "- (tired, get)\n",
    "- (tired, very)\n",
    "- (tired, of)\n",
    "- (tired, sitting)\n",
    "\n",
    "You can think about the model in two ways\n",
    "\n",
    "1. One sample with four targets\n",
    "    - tired -> (very, get, of, sitting)\n",
    "2. Four samples with a single target each\n",
    "    - tired -> very\n",
    "    - tired -> get\n",
    "    - tired -> of\n",
    "    - tired -> sitting\n",
    "\n",
    "\n",
    "Let's define two matrices of weights $W^{(1)} \\in M(V, D)$ that maps a one-hot encoded (sparse) representation of a word to a $D$-dimensional (dense) representation of that word. Let $W^{(2)} \\in M(D, V)$ be a matrix of weights that maps a $D$-dimensional representation of a word to a $V$ dimensional output vector.\n",
    "\n",
    "For a vocabulary of size $V = 3$ and embedding size $D = 2$, the two matrices of weights will be:\n",
    "\n",
    "$$\n",
    "W^{(1)} = \\begin{pmatrix}\n",
    "    w^{(1)}_{11} & w^{(1)}_{12}  \\\\\n",
    "    w^{(1)}_{21} & w^{(1)}_{22} \\\\\n",
    "    w^{(1)}_{31} & w^{(1)}_{32}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The matrix of weights for the output layer will be:\n",
    "\n",
    "$$\n",
    "W^{(2)} = \\begin{pmatrix}\n",
    "    w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13} \\\\\n",
    "    w^{(2)}_{21} & w^{(2)}_{22} & w^{(2)}_{23} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Let's look at a single training sample (center word, context word pair) for the word \"tired\":\n",
    "and let the one-hot encoded representation of the word \"tired\" be:\n",
    "$$\n",
    "(0, 1, 0)\n",
    "$$\n",
    "\n",
    "The hidden layer will be:\n",
    "\n",
    "$$\n",
    "H = (0, 1, 0) \\begin{pmatrix}\n",
    "    w^{(1)}_{11} & w^{(1)}_{12}  \\\\\n",
    "    w^{(1)}_{21} & w^{(1)}_{22} \\\\\n",
    "    w^{(1)}_{31} & w^{(1)}_{32}\n",
    "\\end{pmatrix} = (w^{(1)}_{21}, w^{(1)}_{22})\n",
    "$$\n",
    "\n",
    "Notice that because of the on-hot encoded nature of the input word vector, the hidden layer is just the second row of the matrix $W^{(1)}$. We can use this fact in the software implementation of the model to avoid the matrix multiplication (and save compute time).\n",
    "\n",
    "\n",
    "The output layer will be:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a & = (w^{(1)}_{21}, w^{(1)}_{22}) \\begin{pmatrix}\n",
    "    w^{(2)}_{11} & w^{(2)}_{12} & w^{(2)}_{13} \\\\\n",
    "    w^{(2)}_{21} & w^{(2)}_{22} & w^{(2)}_{23} \\\\\n",
    "\\end{pmatrix} \\\\\n",
    "& = \\begin{pmatrix}\n",
    "  w^{(1)}_{21} w^{(2)}_{11} + w^{(1)}_{22} w^{(2)}_{21} \\\\ \n",
    "  w^{(1)}_{21} w^{(2)}_{12} + w^{(1)}_{22} w^{(2)}_{22} \\\\\n",
    "  w^{(1)}_{21} w^{(2)}_{13} + w^{(1)}_{22} w^{(2)}_{23}) \\\\\n",
    "  \\end{pmatrix}^{T}\n",
    "\\end{align}\n",
    "$$\n",
    "The transpose in the last step is there only for the sake of readability. The output layer is a vector of size $V = 3$ that contains the probabilities of the context words given the center word. The softmax function is applied to the output layer to produce the probabilities:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(a) = \\begin{pmatrix}\n",
    "  \\frac{e^{a_1}}{e^{a_1} + e^{a_2} + e^{a_3}} \\\\\n",
    "  \\frac{e^{a_2}}{e^{a_1} + e^{a_2} + e^{a_3}} \\\\\n",
    "  \\frac{e^{a_3}}{e^{a_1} + e^{a_2} + e^{a_3}} \\\\\n",
    "  \\end{pmatrix} =\n",
    "  \\begin{pmatrix}\n",
    "  p_1 \\\\\n",
    "  p_2 \\\\\n",
    "  p_3 \\\\\n",
    "  \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "These probabilities are the predicted probabilities of the context words given the center word. These probabilities can be compared to the one-hot encoded representation of the context words to compute the loss function and we can apply gradient descent to update the weights.\n",
    "\n",
    "Although theoretically sound, this approach suffers from a couple of practical difficulties.\n",
    "\n",
    "The softmax function is computationally expensive. The softmax function is applied to a vector of size $V$ and involves exponentiation of the elements of the vector. For a large vocabulary size $V$, e.g. a coupe of millions this can be computationally expensive. The pretrained word2vec vectors provided by google have a vocabulary size of about 4 million! This means that a lot oc compute time will be spent on working out the softmax function.\n",
    "\n",
    "The authors of the word2vec models proposed a couple of solutions to this problem. The first solution is to use a hierarchical softmax function instead of the softmax function. The hierarchical softmax function is a binary tree of words where each node is a binary classifier that predicts whether the word is in the left or right branch of the tree. \n",
    "\n",
    "Here we will focus on the second approach that is called negative sampling. \n",
    "\n",
    "\n",
    "## Negative Sampling\n",
    "\n",
    "\n",
    "Let's look at the predictions that skip-gram model makes for a context word. For any given center word, most words in the vocabulary are not context words. For example, for the center word \"tired\", the words \"very\", \"get\", \"of\" and \"sitting\" are context words, but the words \"apple\", \"car\", \"house\" and \"computer\" are not context words. The skip-gram model will predict a high probability for the context words and a low probability for the non-context words.\n",
    "\n",
    "In order to reduce the computational complexity of the softmax function, the authors of the word2vec models proposed to train the model to predict the context words and a small number of random words that are not in the context in a binary classification problem.\n",
    "\n",
    "Instead of doing full multi-class classification with softmax, we can train the model to do binary classification. For each training sample (center word, context word pair), we will create a new training sample where the center word is paired with a random word from the vocabulary that is not in the context. The new training sample will have a label of 1 if the random word is in the context and a label of 0 if the random word is not in the context. We will train the model to predict the label of the new training sample.\n",
    "\n",
    "\n",
    "Let's see an example. Consider the training sample (center word, context word pair) for the word \"tired\" that we saw before. In the following, $\\sigma$ stands for the sigmoid function (which we use to produce probabilities).\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "In our example the logistic regressions for the center word \"tired\" will be:\n",
    "\n",
    "- (tired, very): $p(very | tired) = \\sigma(W^{(1)}_{\\text{very}}W^{o}_{\\text{tired}})$\n",
    "- (tired, get): $p(get | tired) = \\sigma(W^{(1)}_{\\text{get}}W^{o}_{\\text{tired}})$\n",
    "- (tired, very): $p(of | tired) = \\sigma(W^{(1)}_{\\text{of}}W^{o}_{\\text{tired}})$\n",
    "- (tired, very): $p(sitting | tired) = \\sigma(W^{(1)}_{\\text{sitting}}W^{o}_{\\text{tired}})$\n",
    "\n",
    "By sampling from the modified uni-gram distribution we can select a couple (hyperparameter) of negative examples for each center word. For example, we can sample the words \"cat\", \"dog\", \"mouse\", \"rabbit\", and \"horse\" as negative examples for the center word \"tired\". The logistic regressions for the negative examples will be:\n",
    "\n",
    "- (tired, cat): $p(cat | tired) = \\sigma(W^{(1)}_{\\text{cat}}W^{o}_{\\text{tired}})$\n",
    "- (tired, dog): $p(dog | tired) = \\sigma(W^{(1)}_{\\text{dog}}W^{o}_{\\text{tired}})$\n",
    "- (tired, mouse): $p(mouse | tired) = \\sigma(W^{(1)}_{\\text{mouse}}W^{o}_{\\text{tired}})$\n",
    "- (tired, rabbit): $p(rabbit | tired) = \\sigma(W^{(1)}_{\\text{rabbit}}W^{o}_{\\text{tired}})$\n",
    "- (tired, horse): $p(horse | tired) = \\sigma(W^{(1)}_{\\text{horse}}W^{o}_{\\text{tired}})$\n",
    "\n",
    "\n",
    "The loss function with negative sampling is:\n",
    "\n",
    "$$\n",
    "J = \\log p(very | tired) + \\log p(get | tired) + \\log p(of | tired) + \\log p(sitting | tired) + \\log (1 - p(cat | tired)) + \\log (1 - p(dog | tired)) + \\log (1 - p(mouse | tired)) + \\log (1 - p(rabbit | tired)) + \\log (1 - p(horse | tired))\n",
    "$$\n",
    "\n",
    "Generalizing the loss will give us\n",
    "\n",
    "$$\n",
    "J = \\sum_{j \\in \\text{pos. examples}} \\log \\sigma(W^{(1)}_jW^{(2)}) + \\sum_{j \\in \\text{neg. examples}} \\log (1 - \\sigma(W^{(1)}_jW^{(2)}))\n",
    "$$\n",
    "\n",
    "We can use a property of the sigmoid function to simplify the loss function:\n",
    "\n",
    "$$\n",
    "\\sigma(x) + \\sigma(-x) = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "J = \\sum_{j \\in \\text{pos. examples}} \\log \\sigma(W^{(1)}_jW^{(2)}) + \\sum_{j \\in \\text{neg. examples}} \\log (\\sigma(-W^{(1)}_jW^{(2)}))\n",
    "$$\n",
    "\n",
    "\n",
    "## Unigram Distribution\n",
    "\n",
    "We introduced the negative sampling technique to reduce the computational cost of the softmax function but the question remains how to choose the negative examples. The authors of the skip-gram model suggest using the uni-gram distribution raised to the 3/4 power.\n",
    "\n",
    "The uni-gram distribution is simply the frequency of each word in the corpus. Let $f(v)$ be the frequency of the $v$-th word in the corpus. The probability of sampling the $v$-th word is:\n",
    "\n",
    "$$\n",
    "\\text{f}_{3/4}(v) = f(v)^{3/4} \\\\ \n",
    "p_{\\text{neg}}(v) = \\frac{f_{3/4}}{\\sum_{v' = 1}^{V} f_{3/4}(v')}\n",
    "$$\n",
    "\n",
    "The basic idea of raising the frequencies to the 3/4 power is to reduce the probability of sampling frequent words. The authors of the skip-gram model found that this distribution works well empirically, but there is no theoretical justification for it. It is a hyperparameter of the model, and it can be tuned using a validation set (e.g. checking how the word-vectors perform in word analogy tests).\n",
    "\n",
    "\n",
    "## Gradient Descent with Negative Sampling\n",
    "\n",
    "For a set of positive and negative examples, the $t_n$ equal to 1 for positive examples and 0 for negative examples. Furthermore, let $p_n$ be the predicted probability that a word is in the context of the center word.\n",
    "\n",
    "$$\n",
    "p(y = 1 | center) = \\sigma(W^{(2)T}_{target}W^{(1)}_{center}) \\\\\n",
    "$$\n",
    "The loss function for a single training sample is therefore:\n",
    "\n",
    "$$\n",
    "J = - t_{n} \\log (y_n = 1 | center) - (1 - t_{n}) \\log (1 - p(y_n = 1 | center))\n",
    "$$\n",
    "\n",
    "For a batch of $N$ training samples, the loss function is:\n",
    "\n",
    "$$\n",
    "J = - \\sum_{n = 1}^{N} t_{n} \\log p_n + (1 - t_{n}) \\log (1 - p_n)\n",
    "$$\n",
    "\n",
    "In order to understand the weight update process it is easier to consider the loss function for a single training sample: a center word and a target word.\n",
    "\n",
    "$$\n",
    "J_n = - t_{n} \\log p_n + (1 - t_{n}) \\log (1 - p_n)\n",
    "$$\n",
    "\n",
    "\n",
    "In order to use gradient descent to minimize the loss function, we need to compute the gradient of the loss function with respect to the weights. In the following $W^{(2)}_n$ is a $D$-dimensional vector that is the $n$-th column of the weight matrix $W^{(2)}$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial W^{(2)}_n} J & = \\frac{\\partial}{\\partial W^{(2)}_n} \\left( -  t_{n} \\log p_n - (1 - t_{n}) \\log (1 - p_n) \\right) \\\\\n",
    "& = - \\left( \\frac{t_{n}}{p_n} \\frac{\\partial p_n}{\\partial W^{(2)}_n} - \\frac{1 - t_{n}}{1 - p_n} \\frac{\\partial p_n}{\\partial W^{(2)}_n} \\right)\\\\\n",
    "& = - \\left( \\frac{t_{n}}{p_n} - \\frac{1 - t_{n}}{1 - p_n} \\right) \\frac{\\partial p_n}{\\partial W^{(2)}_n}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The probability $p_n$ is just the sigmoid function applied to the net value of the output layer:\n",
    "\n",
    "$$\n",
    "p_n = \\sigma(a_n)\n",
    "$$\n",
    "We can apply the chain rule to compute the derivative of the loss function with respect to the weights:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial p_n}{\\partial a_n} = \\frac{\\partial \\sigma(a_n)}{\\partial a_n} = \\sigma(a_n)(1 - \\sigma(a_n))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial W^{(2)}_n} J_n & = - \\left( \\frac{t_{n}}{p_n} - \\frac{1 - t_{n}}{1 - p_n} \\right) \\frac{\\partial p_n}{\\partial W^{(2)}_n} \\\\\n",
    "& = - \\left( \\frac{t_{n}}{p_n} - \\frac{1 - t_{n}}{1 - p_n} \\right) \\frac{\\partial p_n}{\\partial a_n} \\frac{\\partial a_n}{\\partial W^{(2)}_n} \\\\\n",
    "& = - \\left( \\frac{t_{n}}{p_n} - \\frac{1 - t_{n}}{1 - p_n} \\right) \\sigma(a_n)(1 - \\sigma(a_n)) \\frac{\\partial a_n}{\\partial W^{(2)}_n} \\\\\n",
    "& = - \\left( \\frac{t_{n}}{p_n} - \\frac{1 - t_{n}}{1 - p_n} \\right) p_n(1 - p_n) \\frac{\\partial a_n}{\\partial W^{(2)}_n} \\\\\n",
    "& = - \\left( t_n (1 - p_n) - (1 - t_n)p_n \\right) \\frac{\\partial a_n}{\\partial W^{(2)}_n} \\\\\n",
    "& = - \\left( t_n - t_np_n - p_n + t_np_n \\right) \\frac{\\partial a_n}{\\partial W^{(2)}_n} \\\\\n",
    "& = - \\left( t_n - p_n \\right) \\frac{\\partial a_n}{\\partial W^{(2)}_n} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_n = W^{(2)T}_{n}W^{(1)}_{center}\n",
    "$$\n",
    "The derivative of the net value of the output layer with respect to the weights is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a_n}{\\partial W^{(2)}_n} = W^{(1)}_{center}\n",
    "$$\n",
    "In the end, the gradient of the loss function with respect to the output layer weights is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial W^{(2)}_n} J & = \\left( t_n - p_n \\right) W^{(1)}_{\\text{center}} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In our software implementation we will want to compute the gradient of the loss function for a whole batch of training samples (positive samples and negative samples), so it is \n",
    "convenient (and much faster computationally) to express the gradient for the batch in matrix form.\n",
    "\n",
    "For a batch of $N = 3$ training samples, the gradients could be the following (for example):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{J}{\\partial W^{(2)}_{of}} & = W^{(1)}_{tired} (p_{of} - t_{of}) \\\\\n",
    "\\frac{J}{\\partial W^{(2)}_{sitting}} & = W^{(1)}_{tired} (p_{sitting} - t_{sitting}) \\\\\n",
    "\\frac{J}{\\partial W^{(2)}_{car}} & = W^{(1)}_{tired} (p_{car} - t_{car}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This time we will need to update three ($N$) columns of the weight matrix $W^{(2)}$ that\n",
    "correspond to the target words. We can express the gradient of the loss function\n",
    "with respect to the $N \\times D$ weights that need to be updated as the outer\n",
    "product of the $D$-dimensional vector $W^{(1)}_{tired}$ and the $N$-dimensional\n",
    "vector of errors $P_C - T_C$. With this notation $C$ stands for the set of target\n",
    "words. $\\otimes$ denotes the outer product.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{(2)}_C} = W^{(1)}_{tired} \\otimes (P_{C} - T_{C})\n",
    "$$\n",
    "\n",
    "To recall, the outer product of an $m$-dimensional vector $a$ and an $n$-dimensional vector $b$ is an $m \\times n$ matrix with the following form:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "a_1 \\\\\n",
    "a_2 \\\\\n",
    "\\vdots \\\\\n",
    "a_m\n",
    "\\end{pmatrix}\n",
    "\\otimes \n",
    "\\begin{pmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_n\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "a_1 b_1 & a_1 b_2 & \\cdots & a_1 b_n \\\\\n",
    "a_2 b_1 & a_2 b_2 & \\cdots & a_2 b_n \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_m b_1 & a_m b_2 & \\cdots & a_m b_n \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Next we need to find the gradient of the loss function with respect to the center word vector.\n",
    "As before, we will derive it for a single training sample, and then we will generalize it for a batch of training samples. The good news is that we can reuse the result from the previous section.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial a_n} & = p_n - t_n \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Now we need to see that the loss function $J$ depends on the center word vector $W^{(1)}_{center}$ through net output values of all samples in the batch $a_n$. Therefore, the derivative of the loss function with respect to the center word vector is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial W^{(1)}_{center}} & = \\sum_{n=1}^{N} \\frac{\\partial J}{\\partial a_n} \\frac{\\partial a_n}{\\partial W^{(1)}_{center}} \\\\\n",
    "& = \\sum_{n=1}^{N} (p_n - t_n) \\frac{\\partial}{\\partial W^{(1)}_{center}}\\left(\n",
    "W^{(2)T}_{n}W^{(1)}_{center}\n",
    "\\right) \\\\\n",
    "& = \\sum_{n=1}^{N} (p_n - t_n) W^{(2)}_{n} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## Subsampling Frequent Words\n",
    "\n",
    "There is one final detail of the skip-gram model that we need to discuss.\n",
    "\n",
    "A pattern occurring in most human language texts is that some words are much more frequent than others. There is an empirical finding known as Zipf's law that states that the frequency of a word is inversely proportional to its rank in the frequency table. For example, the most frequent word will occur twice as often as the second most frequent word, three times as often as the third most frequent word, and so on.\n",
    "\n",
    "$$\n",
    "\\text{frequency} \\propto \\frac{1}{\\text{rank}}\n",
    "$$\n",
    "\n",
    "In the English language, \"the\" is by far the most common word. It accounts for nearly 7% of all words in a typical text. The second most common word \"of\" accounts for slightly over 3% of words, followed by \"and\" which accounts for slightly over 2% of words.\n",
    "\n",
    "In the context of the skip-gram model this means that the model will spend a lot of time updating the weights for the word \"the\" and very little time updating the weights for the word \"temporary\" (for example). We would like to spend more time updating the weights for rare words and less time updating the weights for frequent words, especially because the most \n",
    "common words are often function words that do not carry much meaning.\n",
    "\n",
    "A way to work around this problem is to use only a part of the words in a sentence for training in each epoch. We would certainly not like to exclude the same words every time, because this would break the context of the center word. So we will take a random sample of words from the sentence. Thus, for each epoch the sample of words will be different. This is called subsampling.\n",
    "\n",
    "The subsampling distribution proposed in word2vec is the following:\n",
    "\n",
    "$$\n",
    "P(v) = 1 - \\sqrt{\\frac{t}{f(v)}}\n",
    "$$\n",
    "\n",
    "where $t$ is a threshold parameter and $f(v)$ is the frequency of the word $v$ in the training corpus. The threshold parameter $t$ is usually set to $10^{-5}$. Again, this is a hyperparameter that can be tuned.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "190844f20ef41886"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
